import json
import numpy as np
from pathlib import Path
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense

# è¨­å®šå€¤
MAX_SEQUENCE_LENGTH = 10
VOCAB_SIZE = 500

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å ´æ‰€ã‚’åŸºæº–ã«ãƒ‘ã‚¹ã‚’è§£æ±º
BASE_DIR = Path(__file__).resolve().parent.parent

# ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰
with open(BASE_DIR / 'data/qa_data.json', 'r', encoding='utf-8') as f:
    data = json.load(f)
with open(BASE_DIR / 'data/dictionary.json', 'r', encoding='utf-8') as f: # ä¿®æ­£ä¸è¦
    dictionary = json.load(f)

# è¾æ›¸ç½®æ›é–¢æ•°
def apply_dictionary(text, dictionary):
    for key, value in dictionary.items():
        text = text.replace(key, value)
    return text

# è³ªå•ã¨ãƒ©ãƒ™ãƒ«ã®æŠ½å‡ºã€è¾æ›¸ç½®æ›é©ç”¨
questions = []
labels = []
for item in data:
    for q in item['questions']:
        # æ„å›³èªè­˜ç²¾åº¦å‘ä¸Šã®ãŸã‚ã®è¾æ›¸ç½®æ›ã‚’é©ç”¨
        processed_q = apply_dictionary(q, dictionary)
        questions.append(processed_q)
        labels.append(item['intent'])

# ãƒ©ãƒ™ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚° (æ„å›³å -> ID)
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(labels)
Y = to_categorical(integer_encoded)
intent_classes = list(label_encoder.classes_) # JSå´ã§ä½¿ç”¨

# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å­¦ç¿’ã¨ä¿å­˜
tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token="<unk>")
tokenizer.fit_on_texts(questions)

# ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¤‰æ›ã—ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
sequences = tokenizer.texts_to_sequences(questions)
X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')

# ğŸ’¡ é‡è¦ãªã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (JSå´ã§åˆ©ç”¨)
# 1. ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒœã‚­ãƒ£ãƒ–ãƒ©ãƒªã‚’JSONã¨ã—ã¦ä¿å­˜
tokenizer_json = tokenizer.to_json(ensure_ascii=False)
with open(BASE_DIR / 'public/data/tokenizer_config.json', 'w', encoding='utf-8') as f:
    f.write(tokenizer_json)
# 2. æ„å›³ã®ã‚¯ãƒ©ã‚¹åã‚’ä¿å­˜
with open(BASE_DIR / 'public/data/intent_classes.json', 'w', encoding='utf-8') as f:
    json.dump(intent_classes, f, ensure_ascii=False)
    

# ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ (ã‚·ãƒ³ãƒ—ãƒ«ãªBOW/åŸ‹ã‚è¾¼ã¿å¹³å‡ãƒ¢ãƒ‡ãƒ«)
model = Sequential([
    Embedding(VOCAB_SIZE, 16, input_length=MAX_SEQUENCE_LENGTH),
    GlobalAveragePooling1D(), # å…¨å˜èªã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®å¹³å‡ã‚’å–ã‚‹
    Dense(16, activation='relu'),
    Dense(len(intent_classes), activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ (X, Yã¯ä¸Šã§ä½œæˆã—ãŸã‚‚ã®ã‚’ä½¿ç”¨)
model.fit(X, Y, epochs=50, verbose=0)

# å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
saved_model_path = BASE_DIR / "public/model/saved_model"
tfjs_model_path = BASE_DIR / "public/model"
saved_model_path.parent.mkdir(parents=True, exist_ok=True)

# ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ (SavedModelå½¢å¼)
model.save(saved_model_path)

print(f"Model saved to {saved_model_path}")
print("\nTo convert the model to TensorFlow.js format, run the following command:")
print("-" * 70)
print(f"tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model \"{saved_model_path}\" \"{tfjs_model_path}\"")
print("-" * 70)

# SavedModelã‚’tfjså½¢å¼ã«å¤‰æ›
# (ã‚³ãƒãƒ³ãƒ‰ä¾‹)
# tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model "c:\programs\GitHubWS\AI-ChatBot\public\model\saved_model" "c:\programs\GitHubWS\AI-ChatBot\public\model"